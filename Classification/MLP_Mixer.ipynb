{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26145,"status":"ok","timestamp":1679802039051,"user":{"displayName":"Luis Lara","userId":"10661964764539997458"},"user_tz":240},"id":"n9L7NaWxIqT2","outputId":"8199641b-4fdc-494d-af50-8818938200fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["root_dir = \"/content/drive/Othercomputers/Mac/Mila/Winter_2023/ift6759_project/\"\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","\n","from google.colab import drive\n","import sys\n","\n","drive.mount('/content/drive')\n","sys.path.append(root_dir)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6800,"status":"ok","timestamp":1679802045849,"user":{"displayName":"Luis Lara","userId":"10661964764539997458"},"user_tz":240},"id":"8FEA3NFX0fwE"},"outputs":[],"source":["import torch\n","import numpy as np\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.utils.data import Dataset, random_split\n","from PIL import Image\n","import os\n","import pandas as pd\n","\n","# transform = transforms.Compose([\n","#     transforms.RandomRotation(30),      # rotate +/- 30 degrees\n","#     transforms.RandomHorizontalFlip(),  # rHorizontally flip the given image randomly with a given probability (default p=0.5)\n","#     #transforms.RandomVerticalFlip() #Vertically flip the given image randomly with a given probability (default p=0.5), not recommended for medical images\n","#     transforms.Resize((224, 224)),       #  be sure to pass in a list or a tuple\n","#     transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n","#     transforms.RandomAdjustSharpness(1.5, p=0.5), #\n","#     transforms.RandomAdjustSharpness(0.5, p=0.5),\n","#     transforms.RandomAutocontrast(p=0.5),\n","#     transforms.RandomEqualize(p=0.5),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize([0.485, 0.456, 0.406],\n","#                           [0.229, 0.224, 0.225])\n","# ])\n","\n","transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n","  # transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n","  transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n","  transforms.ToTensor(),\n","  transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n","])\n","\n","class CovidSeverityDataset(Dataset):\n","    def __init__(self, root_dir, transform = transform, split_lengths = [0.7, 0.1, 0.2], split_seed = 42, batch_size = 10, shuffle = True, num_workers = [0,0,0]):\n","      self.root_dir = root_dir\n","      self.csv_path = self.root_dir + \"data_processing/combined_cxr_metadata.csv\"\n","      self.data_path = self.root_dir + \"processed_images/\"\n","      self.transform = transform\n","      self.split_lengths = split_lengths\n","      self.split_seed = split_seed\n","      self.batch_size = batch_size\n","      self.shuffle = shuffle\n","      self.num_workers = num_workers\n","      self.dataframe = pd.read_csv(self.csv_path, index_col=0)\n","  \n","    def __len__(self):\n","      return len(self.dataframe)\n","\n","    def __getitem__(self, index):\n","      img_path = os.path.join(self.data_path, self.dataframe.iloc[index, 0])\n","      image = Image.open(img_path)\n","      y_label = torch.tensor(self.dataframe.iloc[index, 1])\n","      if self.transform:\n","        image = self.transform(image)\n","      else:\n","        convert_tensor = transforms.PILToTensor()\n","        image = convert_tensor(image)\n","      return (image, y_label)\n","  \n","    def get_subsets(self):\n","      subsets = random_split(self, self.split_lengths, generator=torch.Generator().manual_seed(self.split_seed))\n","      train = torch.utils.data.DataLoader(dataset=subsets[0], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers[0], drop_last=True, pin_memory=True)\n","      val = torch.utils.data.DataLoader(dataset=subsets[1], batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.num_workers[1])\n","      test = torch.utils.data.DataLoader(dataset=subsets[2], batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.num_workers[2])\n","      return train, val, test\n","\n","\n","# How to use this class?\n","# root_dir = \"/content/drive/MyDrive/Mila/Winter_2023/ift6759_project/\"\n","# dataset = CovidSeverityDataset(root_dir, transform = False)\n","# train, val, test = dataset.get_subsets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pixjTH4YJJ0X"},"outputs":[],"source":["# from covidSeverityDataset import CovidSeverityDataset\n","\n","# dataset = CovidSeverityDataset(root_dir, transform = False, batch_size = 10, shuffle = True, num_workers = [2,0,0])\n","# dataset = CovidSeverityDataset(root_dir, batch_size = 100, shuffle = True, num_workers = [4,2,2])\n","# train, val, test = dataset.get_subsets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"doQrtDCNLuQd"},"outputs":[],"source":["# ludo = next(enumerate(train)) \n","# print(ludo[1])"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1679802045849,"user":{"displayName":"Luis Lara","userId":"10661964764539997458"},"user_tz":240},"id":"twZjsc2yj3BJ"},"outputs":[],"source":["import torch\n","from typing import List, Tuple\n","from torch import nn\n","from torch.nn.parameter import Parameter\n","import torch.nn.functional as F\n","\n","class MLP(torch.nn.Module):\n","    # 20 points\n","    def __init__(self, input_size: int, hidden_sizes: List[int], num_classes: int, activation: str = \"relu\"):\n","        super(MLP, self).__init__() \n","        self.input_size = input_size\n","        self.hidden_sizes = hidden_sizes\n","        assert len(hidden_sizes) > 1, \"You should at least have one hidden layer\"\n","        self.num_classes = num_classes\n","        self.activation = activation\n","        assert activation in ['tanh', 'relu', 'sigmoid'], \"Invalid choice of activation\"\n","        self.hidden_layers, self.output_layer = self._build_layers(input_size, hidden_sizes, num_classes)\n","        \n","        # Initializaton\n","        self._initialize_linear_layer(self.output_layer)\n","        for layer in self.hidden_layers:\n","            self._initialize_linear_layer(layer)\n","    \n","    def _build_layers(self, input_size: int, \n","                        hidden_sizes: List[int], \n","                        num_classes: int) -> Tuple[nn.ModuleList, nn.Module]:\n","        \"\"\"\n","        Build the layers for MLP. Be ware of handlling corner cases.\n","        :param input_size: An int\n","        :param hidden_sizes: A list of ints. E.g., for [32, 32] means two hidden layers with 32 each.\n","        :param num_classes: An int\n","        :Return:\n","            hidden_layers: nn.ModuleList. Within the list, each item has type nn.Module\n","            output_layer: nn.Module\n","        \"\"\"\n","        hidden_layers = nn.ModuleList()\n","        for i, hidden_size in enumerate(hidden_sizes):\n","            if i == 0:\n","                hidden_layers.append(nn.Linear(input_size, hidden_size))\n","            else:\n","                hidden_layers.append(nn.Linear(hidden_sizes[i-1], hidden_size))\n","        output_layer = nn.Linear(hidden_sizes[-1], num_classes)\n","        return hidden_layers, output_layer\n","    \n","    def activation_fn(self, activation, inputs: torch.Tensor) -> torch.Tensor:\n","        \"\"\" process the inputs through different non-linearity function according to activation name \"\"\"\n","        if activation == \"relu\":\n","            return F.relu(inputs)\n","        elif activation == \"tanh\":\n","            return torch.tanh(inputs)\n","        elif activation == \"sigmoid\":\n","            return torch.sigmoid(inputs)\n","        else:\n","            raise ValueError(\"Invalid activation function\")\n","        \n","    def _initialize_linear_layer(self, module: nn.Linear) -> None:\n","        \"\"\" For bias set to zeros. For weights set to glorot normal \"\"\"\n","        nn.init.zeros_(module.bias)\n","        nn.init.xavier_normal_(module.weight)\n","        \n","    def forward(self, images: torch.Tensor) -> torch.Tensor:\n","        \"\"\" Forward images and compute logits.\n","        1. The images are first fattened to vectors. \n","        2. Forward the result to each layer in the self.hidden_layer with activation_fn\n","        3. Finally forward the result to the output_layer.\n","        \n","        :param images: [batch, channels, width, height]\n","        :return logits: [batch, num_classes]\n","        \"\"\"\n","        # 1. The images are first fattened to vectors. \n","        x = images.view(images.size(0), -1)\n","\n","        # 2. Forward the result to each layer in the self.hidden_layer with activation_fn\n","        for layer in self.hidden_layers:\n","            x = self.activation_fn(self.activation, layer(x))\n","\n","        # 3. Finally forward the result to the output_layer.\n","        logits = self.output_layer(x)\n","\n","        return logits\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4776,"status":"ok","timestamp":1679802050622,"user":{"displayName":"Luis Lara","userId":"10661964764539997458"},"user_tz":240},"id":"DIszOOpIjdQP","outputId":"c4ea7668-ffb6-44ab-a4b4-294bf2cf2c2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initialized MLP model with 38570775 total parameters, of which 38570775 are learnable.\n"]}],"source":["from torch import optim\n","\n","model_config = {\n","    # \"input_size\": 3072,\n","    \"input_size\": 150528,\n","    # \"hidden_sizes\": [1024, 512, 64, 64],\n","    \"hidden_sizes\": [256, 128, 16, 16],\n","    \"num_classes\": 7,\n","    \"activation\": \"relu\"\n","}\n","\n","# Optimization\n","optimizer: str = 'adamw'  # [sgd, momentum, adam, adamw]\n","lr: float = 1e-3\n","# momentum: float = 0.9\n","weight_decay: float = 5e-2\n","batch_size: int = 4\n","device = \"cuda\"\n","epochs: int = 25\n","print_every: int = 100\n","\n","model = MLP(**model_config)\n","model.to(device)\n","\n","optimizer = optim.AdamW(\n","    # model.parameters(), lr=lr, weight_decay=weight_decay\n","    model.parameters()\n",")\n","\n","print(\n","    f\"Initialized MLP model with {sum(p.numel() for p in model.parameters())} \"\n","    f\"total parameters, of which {sum(p.numel() for p in model.parameters() if p.requires_grad)} are learnable.\"\n",")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1679802050623,"user":{"displayName":"Luis Lara","userId":"10661964764539997458"},"user_tz":240},"id":"YybBZbp1leCr"},"outputs":[],"source":["def to_device(tensors, device):\n","    if isinstance(tensors, torch.Tensor):\n","        return tensors.to(device=device)\n","    elif isinstance(tensors, dict):\n","        return dict(\n","            (key, to_device(tensor, device)) for (key, tensor) in tensors.items()\n","        )\n","    elif isinstance(tensors, list):\n","        return list(\n","            (to_device(tensors[0], device), to_device(tensors[1], device)))\n","    else:\n","        raise NotImplementedError(\"Unknown type {0}\".format(type(tensors)))\n","\n","\n","# def cross_entropy_loss(logits: torch.Tensor, labels: torch.Tensor):\n","#     \"\"\" Return the mean loss for this batch\n","#     :param logits: [batch_size, num_class]\n","#     :param labels: [batch_size]\n","#     :return loss \n","#     \"\"\"\n","#     # convert labels to one-hot encoding\n","#     one_hot = torch.zeros_like(logits)\n","#     one_hot.scatter_(1, labels.unsqueeze(1), 1)\n","    \n","#     # calculate cross-entropy loss\n","#     log_softmax = logits - torch.logsumexp(logits, dim=1, keepdim=True)\n","#     loss = -torch.sum(one_hot * log_softmax, dim=1).mean()\n","\n","#     return loss\n","\n","def compute_accuracy(logits: torch.Tensor, labels: torch.Tensor):\n","    \"\"\" Compute the accuracy of the batch \"\"\"\n","    acc = (logits.argmax(dim=1) == labels).float().mean()\n","    return acc\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1679802050623,"user":{"displayName":"Luis Lara","userId":"10661964764539997458"},"user_tz":240},"id":"fbtXmgPflLqT"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","import time\n","import os\n","\n","def train(epoch, model, dataloader, optimizer):\n","    model.train()\n","    total_iters = 0\n","    epoch_accuracy = 0\n","    epoch_loss = 0\n","    start_time = time.time()\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for idx, batch in enumerate(dataloader):\n","        batch = to_device(batch, device)\n","        optimizer.zero_grad()\n","        imgs, labels = batch\n","        logits = model(imgs)\n","        # loss = cross_entropy_loss(logits, labels)\n","        labels = labels.to(torch.int64)\n","        loss = criterion(logits, labels)     \n","\n","        acc = compute_accuracy(logits, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","        epoch_accuracy += acc.item() / len(dataloader)\n","        epoch_loss += loss.item() / len(dataloader)\n","        total_iters += 1\n","\n","        if idx % print_every == 0:\n","            tqdm.write(f\"[TRAIN] Epoch: {epoch}, Iter: {idx}, Loss: {loss.item():.5f}\")\n","    tqdm.write(f\"== [TRAIN] Epoch: {epoch}, Accuracy: {epoch_accuracy:.3f} ==>\")\n","    return epoch_loss, epoch_accuracy, time.time() - start_time\n","\n","def evaluate(epoch, model, dataloader, mode=\"val\"):\n","    model.eval()\n","    epoch_accuracy=0\n","    epoch_loss=0\n","    total_iters = 0\n","    start_time = time.time()\n","    criterion = nn.CrossEntropyLoss()\n","\n","    with torch.no_grad():\n","        for idx, batch in enumerate(dataloader):\n","            batch = to_device(batch, device)\n","            imgs, labels = batch\n","            logits = model(imgs)\n","            # loss = cross_entropy_loss(logits, labels)\n","            labels = labels.to(torch.int64)\n","            loss = criterion(logits, labels)   \n","            acc = compute_accuracy(logits, labels)\n","            epoch_accuracy += acc.item() / len(dataloader)\n","            epoch_loss += loss.item() / len(dataloader)\n","            total_iters += 1\n","            if idx % print_every == 0:\n","                tqdm.write(\n","                    f\"[{mode.upper()}] Epoch: {epoch}, Iter: {idx}, Loss: {loss.item():.5f}\"\n","                )\n","        tqdm.write(\n","            f\"=== [{mode.upper()}] Epoch: {epoch}, Iter: {idx}, Accuracy: {epoch_accuracy:.3f} ===>\"\n","        )\n","    return epoch_loss, epoch_accuracy, time.time() - start_time\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1679785191682,"user":{"displayName":"Luis Lara","userId":"10661964764539997458"},"user_tz":240},"id":"rFyvqNfaq_hE","outputId":"310b8ebe-e34b-4a50-ed06-67e53a961abb"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["# subsets = torch.utils.data.random_split(dataset, [0.7, 0.1, 0.2], generator=torch.Generator().manual_seed(42))\n","# train_loader = torch.utils.data.DataLoader(subsets[0], batch_size=128, shuffle=True, num_workers=5)\n","# ludo = next(enumerate(train_loader))\n","# print(type(ludo[1][1][0]))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2176,"status":"ok","timestamp":1679802052796,"user":{"displayName":"Luis Lara","userId":"10661964764539997458"},"user_tz":240},"id":"HNRVY0BB_kWu"},"outputs":[],"source":["dataset = CovidSeverityDataset(root_dir, batch_size = 10, shuffle = True, num_workers = [0,0,0])\n","train_loader, val, test = dataset.get_subsets()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"gxB-E2j77p-3"},"outputs":[{"name":"stdout","output_type":"stream","text":["====== Epoch 0 ======>\n","[TRAIN] Epoch: 0, Iter: 0, Loss: 1.98694\n","[TRAIN] Epoch: 0, Iter: 100, Loss: 1.90601\n","[TRAIN] Epoch: 0, Iter: 200, Loss: 1.82309\n","[TRAIN] Epoch: 0, Iter: 300, Loss: 1.84193\n","[TRAIN] Epoch: 0, Iter: 400, Loss: 1.79679\n","[TRAIN] Epoch: 0, Iter: 500, Loss: 1.73221\n","== [TRAIN] Epoch: 0, Accuracy: 0.267 ==>\n","[VAL] Epoch: 0, Iter: 0, Loss: 1.68666\n","=== [VAL] Epoch: 0, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 1 ======>\n","[TRAIN] Epoch: 1, Iter: 0, Loss: 1.70409\n","[TRAIN] Epoch: 1, Iter: 100, Loss: 1.65994\n","[TRAIN] Epoch: 1, Iter: 200, Loss: 1.60702\n","[TRAIN] Epoch: 1, Iter: 300, Loss: 1.96063\n","[TRAIN] Epoch: 1, Iter: 400, Loss: 1.53165\n","[TRAIN] Epoch: 1, Iter: 500, Loss: 1.74248\n","== [TRAIN] Epoch: 1, Accuracy: 0.306 ==>\n","[VAL] Epoch: 1, Iter: 0, Loss: 1.54446\n","=== [VAL] Epoch: 1, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 2 ======>\n","[TRAIN] Epoch: 2, Iter: 0, Loss: 1.50636\n","[TRAIN] Epoch: 2, Iter: 100, Loss: 1.53105\n","[TRAIN] Epoch: 2, Iter: 200, Loss: 1.92881\n","[TRAIN] Epoch: 2, Iter: 300, Loss: 1.46402\n","[TRAIN] Epoch: 2, Iter: 400, Loss: 1.90726\n","[TRAIN] Epoch: 2, Iter: 500, Loss: 1.62804\n","== [TRAIN] Epoch: 2, Accuracy: 0.316 ==>\n","[VAL] Epoch: 2, Iter: 0, Loss: 1.46846\n","=== [VAL] Epoch: 2, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 3 ======>\n","[TRAIN] Epoch: 3, Iter: 0, Loss: 1.63297\n","[TRAIN] Epoch: 3, Iter: 100, Loss: 1.62738\n","[TRAIN] Epoch: 3, Iter: 200, Loss: 1.59391\n","[TRAIN] Epoch: 3, Iter: 300, Loss: 1.36504\n","[TRAIN] Epoch: 3, Iter: 400, Loss: 1.46790\n","[TRAIN] Epoch: 3, Iter: 500, Loss: 1.44258\n","== [TRAIN] Epoch: 3, Accuracy: 0.316 ==>\n","[VAL] Epoch: 3, Iter: 0, Loss: 1.42948\n","=== [VAL] Epoch: 3, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 4 ======>\n","[TRAIN] Epoch: 4, Iter: 0, Loss: 1.62018\n","[TRAIN] Epoch: 4, Iter: 100, Loss: 1.39485\n","[TRAIN] Epoch: 4, Iter: 200, Loss: 1.34738\n","[TRAIN] Epoch: 4, Iter: 300, Loss: 1.49548\n","[TRAIN] Epoch: 4, Iter: 400, Loss: 1.49289\n","[TRAIN] Epoch: 4, Iter: 500, Loss: 1.59748\n","== [TRAIN] Epoch: 4, Accuracy: 0.316 ==>\n","[VAL] Epoch: 4, Iter: 0, Loss: 1.40952\n","=== [VAL] Epoch: 4, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 5 ======>\n","[TRAIN] Epoch: 5, Iter: 0, Loss: 1.41553\n","[TRAIN] Epoch: 5, Iter: 100, Loss: 1.52260\n","[TRAIN] Epoch: 5, Iter: 200, Loss: 1.61513\n","[TRAIN] Epoch: 5, Iter: 300, Loss: 1.51306\n","[TRAIN] Epoch: 5, Iter: 400, Loss: 1.70429\n","[TRAIN] Epoch: 5, Iter: 500, Loss: 1.61099\n","== [TRAIN] Epoch: 5, Accuracy: 0.316 ==>\n","[VAL] Epoch: 5, Iter: 0, Loss: 1.39913\n","=== [VAL] Epoch: 5, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 6 ======>\n","[TRAIN] Epoch: 6, Iter: 0, Loss: 1.95319\n","[TRAIN] Epoch: 6, Iter: 100, Loss: 1.50009\n","[TRAIN] Epoch: 6, Iter: 200, Loss: 1.79617\n","[TRAIN] Epoch: 6, Iter: 300, Loss: 1.35576\n","[TRAIN] Epoch: 6, Iter: 400, Loss: 1.55166\n","[TRAIN] Epoch: 6, Iter: 500, Loss: 1.54892\n","== [TRAIN] Epoch: 6, Accuracy: 0.316 ==>\n","[VAL] Epoch: 6, Iter: 0, Loss: 1.39188\n","=== [VAL] Epoch: 6, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 7 ======>\n","[TRAIN] Epoch: 7, Iter: 0, Loss: 1.63766\n","[TRAIN] Epoch: 7, Iter: 100, Loss: 1.86415\n","[TRAIN] Epoch: 7, Iter: 200, Loss: 1.62761\n","[TRAIN] Epoch: 7, Iter: 300, Loss: 1.57503\n","[TRAIN] Epoch: 7, Iter: 400, Loss: 1.52893\n","[TRAIN] Epoch: 7, Iter: 500, Loss: 1.59635\n","== [TRAIN] Epoch: 7, Accuracy: 0.316 ==>\n","[VAL] Epoch: 7, Iter: 0, Loss: 1.39060\n","=== [VAL] Epoch: 7, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 8 ======>\n","[TRAIN] Epoch: 8, Iter: 0, Loss: 1.26079\n","[TRAIN] Epoch: 8, Iter: 100, Loss: 1.29090\n","[TRAIN] Epoch: 8, Iter: 200, Loss: 1.50237\n","[TRAIN] Epoch: 8, Iter: 300, Loss: 1.40847\n","[TRAIN] Epoch: 8, Iter: 400, Loss: 1.49255\n","[TRAIN] Epoch: 8, Iter: 500, Loss: 1.53254\n","== [TRAIN] Epoch: 8, Accuracy: 0.316 ==>\n","[VAL] Epoch: 8, Iter: 0, Loss: 1.39082\n","=== [VAL] Epoch: 8, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 9 ======>\n","[TRAIN] Epoch: 9, Iter: 0, Loss: 1.37517\n","[TRAIN] Epoch: 9, Iter: 100, Loss: 1.72258\n","[TRAIN] Epoch: 9, Iter: 200, Loss: 1.47889\n","[TRAIN] Epoch: 9, Iter: 300, Loss: 1.70946\n","[TRAIN] Epoch: 9, Iter: 400, Loss: 1.36600\n","[TRAIN] Epoch: 9, Iter: 500, Loss: 1.84140\n","== [TRAIN] Epoch: 9, Accuracy: 0.316 ==>\n","[VAL] Epoch: 9, Iter: 0, Loss: 1.39094\n","=== [VAL] Epoch: 9, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 10 ======>\n","[TRAIN] Epoch: 10, Iter: 0, Loss: 1.90172\n","[TRAIN] Epoch: 10, Iter: 100, Loss: 1.46328\n","[TRAIN] Epoch: 10, Iter: 200, Loss: 1.63391\n","[TRAIN] Epoch: 10, Iter: 300, Loss: 1.41281\n","[TRAIN] Epoch: 10, Iter: 400, Loss: 1.57194\n","[TRAIN] Epoch: 10, Iter: 500, Loss: 2.12028\n","== [TRAIN] Epoch: 10, Accuracy: 0.316 ==>\n","[VAL] Epoch: 10, Iter: 0, Loss: 1.38732\n","=== [VAL] Epoch: 10, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 11 ======>\n","[TRAIN] Epoch: 11, Iter: 0, Loss: 1.63798\n","[TRAIN] Epoch: 11, Iter: 100, Loss: 1.56732\n","[TRAIN] Epoch: 11, Iter: 200, Loss: 1.20052\n","[TRAIN] Epoch: 11, Iter: 300, Loss: 1.83459\n","[TRAIN] Epoch: 11, Iter: 400, Loss: 1.31505\n","[TRAIN] Epoch: 11, Iter: 500, Loss: 1.39009\n","== [TRAIN] Epoch: 11, Accuracy: 0.316 ==>\n","[VAL] Epoch: 11, Iter: 0, Loss: 1.38996\n","=== [VAL] Epoch: 11, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 12 ======>\n","[TRAIN] Epoch: 12, Iter: 0, Loss: 1.48232\n","[TRAIN] Epoch: 12, Iter: 100, Loss: 1.70529\n","[TRAIN] Epoch: 12, Iter: 200, Loss: 1.62893\n","[TRAIN] Epoch: 12, Iter: 300, Loss: 1.66828\n","[TRAIN] Epoch: 12, Iter: 400, Loss: 1.51273\n","[TRAIN] Epoch: 12, Iter: 500, Loss: 1.35417\n","== [TRAIN] Epoch: 12, Accuracy: 0.316 ==>\n","[VAL] Epoch: 12, Iter: 0, Loss: 1.38958\n","=== [VAL] Epoch: 12, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 13 ======>\n","[TRAIN] Epoch: 13, Iter: 0, Loss: 1.57916\n","[TRAIN] Epoch: 13, Iter: 100, Loss: 1.78677\n","[TRAIN] Epoch: 13, Iter: 200, Loss: 1.95289\n","[TRAIN] Epoch: 13, Iter: 300, Loss: 1.61988\n","[TRAIN] Epoch: 13, Iter: 400, Loss: 1.48262\n","[TRAIN] Epoch: 13, Iter: 500, Loss: 1.65662\n","== [TRAIN] Epoch: 13, Accuracy: 0.316 ==>\n","[VAL] Epoch: 13, Iter: 0, Loss: 1.38848\n","=== [VAL] Epoch: 13, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 14 ======>\n","[TRAIN] Epoch: 14, Iter: 0, Loss: 1.83822\n","[TRAIN] Epoch: 14, Iter: 100, Loss: 1.93315\n","[TRAIN] Epoch: 14, Iter: 200, Loss: 1.38812\n","[TRAIN] Epoch: 14, Iter: 300, Loss: 2.03272\n","[TRAIN] Epoch: 14, Iter: 400, Loss: 1.70856\n","[TRAIN] Epoch: 14, Iter: 500, Loss: 1.35820\n","== [TRAIN] Epoch: 14, Accuracy: 0.316 ==>\n","[VAL] Epoch: 14, Iter: 0, Loss: 1.38924\n","=== [VAL] Epoch: 14, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 15 ======>\n","[TRAIN] Epoch: 15, Iter: 0, Loss: 1.66601\n","[TRAIN] Epoch: 15, Iter: 100, Loss: 1.40482\n","[TRAIN] Epoch: 15, Iter: 200, Loss: 1.57131\n","[TRAIN] Epoch: 15, Iter: 300, Loss: 1.33812\n","[TRAIN] Epoch: 15, Iter: 400, Loss: 1.70024\n","[TRAIN] Epoch: 15, Iter: 500, Loss: 1.67301\n","== [TRAIN] Epoch: 15, Accuracy: 0.316 ==>\n","[VAL] Epoch: 15, Iter: 0, Loss: 1.38681\n","=== [VAL] Epoch: 15, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 16 ======>\n","[TRAIN] Epoch: 16, Iter: 0, Loss: 1.56653\n","[TRAIN] Epoch: 16, Iter: 100, Loss: 1.99495\n","[TRAIN] Epoch: 16, Iter: 200, Loss: 1.77894\n","[TRAIN] Epoch: 16, Iter: 300, Loss: 1.78309\n","[TRAIN] Epoch: 16, Iter: 400, Loss: 1.56662\n","[TRAIN] Epoch: 16, Iter: 500, Loss: 1.50465\n","== [TRAIN] Epoch: 16, Accuracy: 0.316 ==>\n","[VAL] Epoch: 16, Iter: 0, Loss: 1.39050\n","=== [VAL] Epoch: 16, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 17 ======>\n","[TRAIN] Epoch: 17, Iter: 0, Loss: 1.67732\n","[TRAIN] Epoch: 17, Iter: 100, Loss: 1.67901\n","[TRAIN] Epoch: 17, Iter: 200, Loss: 1.54014\n","[TRAIN] Epoch: 17, Iter: 300, Loss: 1.78076\n","[TRAIN] Epoch: 17, Iter: 400, Loss: 1.67016\n","[TRAIN] Epoch: 17, Iter: 500, Loss: 1.36880\n","== [TRAIN] Epoch: 17, Accuracy: 0.316 ==>\n","[VAL] Epoch: 17, Iter: 0, Loss: 1.38824\n","=== [VAL] Epoch: 17, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 18 ======>\n","[TRAIN] Epoch: 18, Iter: 0, Loss: 1.52061\n","[TRAIN] Epoch: 18, Iter: 100, Loss: 1.70176\n","[TRAIN] Epoch: 18, Iter: 200, Loss: 1.58100\n","[TRAIN] Epoch: 18, Iter: 300, Loss: 1.35115\n","[TRAIN] Epoch: 18, Iter: 400, Loss: 1.73284\n","[TRAIN] Epoch: 18, Iter: 500, Loss: 2.06912\n","== [TRAIN] Epoch: 18, Accuracy: 0.316 ==>\n","[VAL] Epoch: 18, Iter: 0, Loss: 1.38662\n","=== [VAL] Epoch: 18, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 19 ======>\n","[TRAIN] Epoch: 19, Iter: 0, Loss: 1.89462\n","[TRAIN] Epoch: 19, Iter: 100, Loss: 1.76264\n","[TRAIN] Epoch: 19, Iter: 200, Loss: 1.89128\n","[TRAIN] Epoch: 19, Iter: 300, Loss: 1.77502\n","[TRAIN] Epoch: 19, Iter: 400, Loss: 1.64040\n","[TRAIN] Epoch: 19, Iter: 500, Loss: 1.42870\n","== [TRAIN] Epoch: 19, Accuracy: 0.316 ==>\n","[VAL] Epoch: 19, Iter: 0, Loss: 1.38627\n","=== [VAL] Epoch: 19, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 20 ======>\n","[TRAIN] Epoch: 20, Iter: 0, Loss: 1.87430\n","[TRAIN] Epoch: 20, Iter: 100, Loss: 1.62485\n","[TRAIN] Epoch: 20, Iter: 200, Loss: 1.39216\n","[TRAIN] Epoch: 20, Iter: 300, Loss: 1.44228\n","[TRAIN] Epoch: 20, Iter: 400, Loss: 1.50103\n","[TRAIN] Epoch: 20, Iter: 500, Loss: 1.70997\n","== [TRAIN] Epoch: 20, Accuracy: 0.316 ==>\n","[VAL] Epoch: 20, Iter: 0, Loss: 1.38631\n","=== [VAL] Epoch: 20, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 21 ======>\n","[TRAIN] Epoch: 21, Iter: 0, Loss: 1.55146\n","[TRAIN] Epoch: 21, Iter: 100, Loss: 1.57165\n","[TRAIN] Epoch: 21, Iter: 200, Loss: 1.63917\n","[TRAIN] Epoch: 21, Iter: 300, Loss: 1.79102\n","[TRAIN] Epoch: 21, Iter: 400, Loss: 1.41454\n","[TRAIN] Epoch: 21, Iter: 500, Loss: 1.61521\n","== [TRAIN] Epoch: 21, Accuracy: 0.316 ==>\n","[VAL] Epoch: 21, Iter: 0, Loss: 1.38711\n","=== [VAL] Epoch: 21, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 22 ======>\n","[TRAIN] Epoch: 22, Iter: 0, Loss: 1.35609\n","[TRAIN] Epoch: 22, Iter: 100, Loss: 1.46834\n","[TRAIN] Epoch: 22, Iter: 200, Loss: 1.75799\n","[TRAIN] Epoch: 22, Iter: 300, Loss: 1.71460\n","[TRAIN] Epoch: 22, Iter: 400, Loss: 1.41505\n","[TRAIN] Epoch: 22, Iter: 500, Loss: 1.54741\n","== [TRAIN] Epoch: 22, Accuracy: 0.316 ==>\n","[VAL] Epoch: 22, Iter: 0, Loss: 1.38907\n","=== [VAL] Epoch: 22, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 23 ======>\n","[TRAIN] Epoch: 23, Iter: 0, Loss: 1.56782\n","[TRAIN] Epoch: 23, Iter: 100, Loss: 1.73677\n","[TRAIN] Epoch: 23, Iter: 200, Loss: 1.58811\n","[TRAIN] Epoch: 23, Iter: 300, Loss: 1.67915\n","[TRAIN] Epoch: 23, Iter: 400, Loss: 1.63590\n","[TRAIN] Epoch: 23, Iter: 500, Loss: 1.84875\n","== [TRAIN] Epoch: 23, Accuracy: 0.316 ==>\n","[VAL] Epoch: 23, Iter: 0, Loss: 1.39110\n","=== [VAL] Epoch: 23, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 24 ======>\n","[TRAIN] Epoch: 24, Iter: 0, Loss: 1.66226\n","[TRAIN] Epoch: 24, Iter: 100, Loss: 1.65420\n","[TRAIN] Epoch: 24, Iter: 200, Loss: 1.54264\n","[TRAIN] Epoch: 24, Iter: 300, Loss: 1.70177\n","[TRAIN] Epoch: 24, Iter: 400, Loss: 1.80463\n","[TRAIN] Epoch: 24, Iter: 500, Loss: 1.50124\n","== [TRAIN] Epoch: 24, Accuracy: 0.316 ==>\n","[VAL] Epoch: 24, Iter: 0, Loss: 1.39115\n","=== [VAL] Epoch: 24, Iter: 71, Accuracy: 0.302 ===>\n","[TEST] Epoch: 24, Iter: 0, Loss: 1.66597\n","[TEST] Epoch: 24, Iter: 100, Loss: 1.46205\n","=== [TEST] Epoch: 24, Iter: 143, Accuracy: 0.333 ===>\n"]}],"source":["from tqdm import tqdm\n","\n","train_losses, valid_losses = [], []\n","train_accs, valid_accs = [], []\n","train_times, valid_times = [], []\n","\n","for epoch in range(epochs):\n","    tqdm.write(f\"====== Epoch {epoch} ======>\")\n","    loss, acc, wall_time = train(epoch, model, train_loader, optimizer)\n","    train_losses.append(loss)\n","    train_accs.append(acc)\n","    train_times.append(wall_time)\n","\n","    loss, acc, wall_time = evaluate(epoch, model, val)\n","    valid_losses.append(loss)\n","    valid_accs.append(acc)\n","    valid_times.append(wall_time)\n","\n","test_loss, test_acc, test_time = evaluate(\n","    epoch, model, test, mode=\"test\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1334517,"status":"ok","timestamp":1679800069392,"user":{"displayName":"Luis Lara","userId":"10661964764539997458"},"user_tz":240},"id":"Dsz-7mJVjzMI","outputId":"68a985b5-80cc-4b43-c31b-f3185b74e304"},"outputs":[{"name":"stdout","output_type":"stream","text":["====== Epoch 0 ======>\n","[TRAIN] Epoch: 0, Iter: 0, Loss: 1.77243\n","[TRAIN] Epoch: 0, Iter: 100, Loss: 2.43915\n","[TRAIN] Epoch: 0, Iter: 200, Loss: 1.75703\n","[TRAIN] Epoch: 0, Iter: 300, Loss: 1.57284\n","[TRAIN] Epoch: 0, Iter: 400, Loss: 1.50404\n","[TRAIN] Epoch: 0, Iter: 500, Loss: 1.50570\n","== [TRAIN] Epoch: 0, Accuracy: 0.299 ==>\n","[VAL] Epoch: 0, Iter: 0, Loss: 1.44679\n","=== [VAL] Epoch: 0, Iter: 71, Accuracy: 0.303 ===>\n","====== Epoch 1 ======>\n","[TRAIN] Epoch: 1, Iter: 0, Loss: 1.30168\n","[TRAIN] Epoch: 1, Iter: 100, Loss: 1.43252\n","[TRAIN] Epoch: 1, Iter: 200, Loss: 1.51021\n","[TRAIN] Epoch: 1, Iter: 300, Loss: 2.13241\n","[TRAIN] Epoch: 1, Iter: 400, Loss: 1.79479\n","[TRAIN] Epoch: 1, Iter: 500, Loss: 1.63138\n","== [TRAIN] Epoch: 1, Accuracy: 0.316 ==>\n","[VAL] Epoch: 1, Iter: 0, Loss: 1.40533\n","=== [VAL] Epoch: 1, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 2 ======>\n","[TRAIN] Epoch: 2, Iter: 0, Loss: 2.17518\n","[TRAIN] Epoch: 2, Iter: 100, Loss: 1.49345\n","[TRAIN] Epoch: 2, Iter: 200, Loss: 1.70677\n","[TRAIN] Epoch: 2, Iter: 300, Loss: 1.61261\n","[TRAIN] Epoch: 2, Iter: 400, Loss: 1.32353\n","[TRAIN] Epoch: 2, Iter: 500, Loss: 1.25849\n","== [TRAIN] Epoch: 2, Accuracy: 0.315 ==>\n","[VAL] Epoch: 2, Iter: 0, Loss: 1.39619\n","=== [VAL] Epoch: 2, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 3 ======>\n","[TRAIN] Epoch: 3, Iter: 0, Loss: 1.47088\n","[TRAIN] Epoch: 3, Iter: 100, Loss: 1.46083\n","[TRAIN] Epoch: 3, Iter: 200, Loss: 1.68418\n","[TRAIN] Epoch: 3, Iter: 300, Loss: 1.52981\n","[TRAIN] Epoch: 3, Iter: 400, Loss: 1.83562\n","[TRAIN] Epoch: 3, Iter: 500, Loss: 1.43778\n","== [TRAIN] Epoch: 3, Accuracy: 0.315 ==>\n","[VAL] Epoch: 3, Iter: 0, Loss: 1.39243\n","=== [VAL] Epoch: 3, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 4 ======>\n","[TRAIN] Epoch: 4, Iter: 0, Loss: 1.61607\n","[TRAIN] Epoch: 4, Iter: 100, Loss: 1.45069\n","[TRAIN] Epoch: 4, Iter: 200, Loss: 2.06899\n","[TRAIN] Epoch: 4, Iter: 300, Loss: 1.51539\n","[TRAIN] Epoch: 4, Iter: 400, Loss: 1.35450\n","[TRAIN] Epoch: 4, Iter: 500, Loss: 1.63189\n","== [TRAIN] Epoch: 4, Accuracy: 0.316 ==>\n","[VAL] Epoch: 4, Iter: 0, Loss: 1.39022\n","=== [VAL] Epoch: 4, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 5 ======>\n","[TRAIN] Epoch: 5, Iter: 0, Loss: 1.37449\n","[TRAIN] Epoch: 5, Iter: 100, Loss: 1.73145\n","[TRAIN] Epoch: 5, Iter: 200, Loss: 1.48873\n","[TRAIN] Epoch: 5, Iter: 300, Loss: 1.97886\n","[TRAIN] Epoch: 5, Iter: 400, Loss: 1.55196\n","[TRAIN] Epoch: 5, Iter: 500, Loss: 1.46129\n","== [TRAIN] Epoch: 5, Accuracy: 0.316 ==>\n","[VAL] Epoch: 5, Iter: 0, Loss: 1.39081\n","=== [VAL] Epoch: 5, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 6 ======>\n","[TRAIN] Epoch: 6, Iter: 0, Loss: 1.93780\n","[TRAIN] Epoch: 6, Iter: 100, Loss: 1.55243\n","[TRAIN] Epoch: 6, Iter: 200, Loss: 1.44727\n","[TRAIN] Epoch: 6, Iter: 300, Loss: 1.99463\n","[TRAIN] Epoch: 6, Iter: 400, Loss: 1.62048\n","[TRAIN] Epoch: 6, Iter: 500, Loss: 1.79115\n","== [TRAIN] Epoch: 6, Accuracy: 0.316 ==>\n","[VAL] Epoch: 6, Iter: 0, Loss: 1.39180\n","=== [VAL] Epoch: 6, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 7 ======>\n","[TRAIN] Epoch: 7, Iter: 0, Loss: 1.30968\n","[TRAIN] Epoch: 7, Iter: 100, Loss: 1.31300\n","[TRAIN] Epoch: 7, Iter: 200, Loss: 1.52795\n","[TRAIN] Epoch: 7, Iter: 300, Loss: 1.67764\n","[TRAIN] Epoch: 7, Iter: 400, Loss: 1.53119\n","[TRAIN] Epoch: 7, Iter: 500, Loss: 1.83854\n","== [TRAIN] Epoch: 7, Accuracy: 0.316 ==>\n","[VAL] Epoch: 7, Iter: 0, Loss: 1.38960\n","=== [VAL] Epoch: 7, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 8 ======>\n","[TRAIN] Epoch: 8, Iter: 0, Loss: 1.69887\n","[TRAIN] Epoch: 8, Iter: 100, Loss: 1.72807\n","[TRAIN] Epoch: 8, Iter: 200, Loss: 1.46704\n","[TRAIN] Epoch: 8, Iter: 300, Loss: 1.46217\n","[TRAIN] Epoch: 8, Iter: 400, Loss: 1.41307\n","[TRAIN] Epoch: 8, Iter: 500, Loss: 1.60246\n","== [TRAIN] Epoch: 8, Accuracy: 0.316 ==>\n","[VAL] Epoch: 8, Iter: 0, Loss: 1.39583\n","=== [VAL] Epoch: 8, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 9 ======>\n","[TRAIN] Epoch: 9, Iter: 0, Loss: 1.67456\n","[TRAIN] Epoch: 9, Iter: 100, Loss: 1.51578\n","[TRAIN] Epoch: 9, Iter: 200, Loss: 1.54952\n","[TRAIN] Epoch: 9, Iter: 300, Loss: 1.67374\n","[TRAIN] Epoch: 9, Iter: 400, Loss: 1.59167\n","[TRAIN] Epoch: 9, Iter: 500, Loss: 1.51512\n","== [TRAIN] Epoch: 9, Accuracy: 0.316 ==>\n","[VAL] Epoch: 9, Iter: 0, Loss: 1.38938\n","=== [VAL] Epoch: 9, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 10 ======>\n","[TRAIN] Epoch: 10, Iter: 0, Loss: 1.73480\n","[TRAIN] Epoch: 10, Iter: 100, Loss: 1.54862\n","[TRAIN] Epoch: 10, Iter: 200, Loss: 1.49989\n","[TRAIN] Epoch: 10, Iter: 300, Loss: 1.29163\n","[TRAIN] Epoch: 10, Iter: 400, Loss: 1.56703\n","[TRAIN] Epoch: 10, Iter: 500, Loss: 2.11380\n","== [TRAIN] Epoch: 10, Accuracy: 0.316 ==>\n","[VAL] Epoch: 10, Iter: 0, Loss: 1.38569\n","=== [VAL] Epoch: 10, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 11 ======>\n","[TRAIN] Epoch: 11, Iter: 0, Loss: 1.69434\n","[TRAIN] Epoch: 11, Iter: 100, Loss: 1.70079\n","[TRAIN] Epoch: 11, Iter: 200, Loss: 1.59266\n","[TRAIN] Epoch: 11, Iter: 300, Loss: 1.67748\n","[TRAIN] Epoch: 11, Iter: 400, Loss: 1.62892\n","[TRAIN] Epoch: 11, Iter: 500, Loss: 1.30549\n","== [TRAIN] Epoch: 11, Accuracy: 0.316 ==>\n","[VAL] Epoch: 11, Iter: 0, Loss: 1.38783\n","=== [VAL] Epoch: 11, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 12 ======>\n","[TRAIN] Epoch: 12, Iter: 0, Loss: 1.46769\n","[TRAIN] Epoch: 12, Iter: 100, Loss: 1.54000\n","[TRAIN] Epoch: 12, Iter: 200, Loss: 1.66363\n","[TRAIN] Epoch: 12, Iter: 300, Loss: 2.06962\n","[TRAIN] Epoch: 12, Iter: 400, Loss: 1.55679\n","[TRAIN] Epoch: 12, Iter: 500, Loss: 1.30634\n","== [TRAIN] Epoch: 12, Accuracy: 0.316 ==>\n","[VAL] Epoch: 12, Iter: 0, Loss: 1.38878\n","=== [VAL] Epoch: 12, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 13 ======>\n","[TRAIN] Epoch: 13, Iter: 0, Loss: 1.76354\n","[TRAIN] Epoch: 13, Iter: 100, Loss: 1.53784\n","[TRAIN] Epoch: 13, Iter: 200, Loss: 1.46887\n","[TRAIN] Epoch: 13, Iter: 300, Loss: 1.35791\n","[TRAIN] Epoch: 13, Iter: 400, Loss: 1.37579\n","[TRAIN] Epoch: 13, Iter: 500, Loss: 1.50005\n","== [TRAIN] Epoch: 13, Accuracy: 0.316 ==>\n","[VAL] Epoch: 13, Iter: 0, Loss: 1.38846\n","=== [VAL] Epoch: 13, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 14 ======>\n","[TRAIN] Epoch: 14, Iter: 0, Loss: 1.44374\n","[TRAIN] Epoch: 14, Iter: 100, Loss: 1.38766\n","[TRAIN] Epoch: 14, Iter: 200, Loss: 1.45882\n","[TRAIN] Epoch: 14, Iter: 300, Loss: 1.94945\n","[TRAIN] Epoch: 14, Iter: 400, Loss: 1.62960\n","[TRAIN] Epoch: 14, Iter: 500, Loss: 1.46012\n","== [TRAIN] Epoch: 14, Accuracy: 0.316 ==>\n","[VAL] Epoch: 14, Iter: 0, Loss: 1.38633\n","=== [VAL] Epoch: 14, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 15 ======>\n","[TRAIN] Epoch: 15, Iter: 0, Loss: 1.66406\n","[TRAIN] Epoch: 15, Iter: 100, Loss: 1.55772\n","[TRAIN] Epoch: 15, Iter: 200, Loss: 1.48501\n","[TRAIN] Epoch: 15, Iter: 300, Loss: 1.85638\n","[TRAIN] Epoch: 15, Iter: 400, Loss: 1.79911\n","[TRAIN] Epoch: 15, Iter: 500, Loss: 1.41298\n","== [TRAIN] Epoch: 15, Accuracy: 0.316 ==>\n","[VAL] Epoch: 15, Iter: 0, Loss: 1.38849\n","=== [VAL] Epoch: 15, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 16 ======>\n","[TRAIN] Epoch: 16, Iter: 0, Loss: 1.66763\n","[TRAIN] Epoch: 16, Iter: 100, Loss: 1.52227\n","[TRAIN] Epoch: 16, Iter: 200, Loss: 1.65707\n","[TRAIN] Epoch: 16, Iter: 300, Loss: 1.57056\n","[TRAIN] Epoch: 16, Iter: 400, Loss: 1.44340\n","[TRAIN] Epoch: 16, Iter: 500, Loss: 1.62454\n","== [TRAIN] Epoch: 16, Accuracy: 0.316 ==>\n","[VAL] Epoch: 16, Iter: 0, Loss: 1.39177\n","=== [VAL] Epoch: 16, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 17 ======>\n","[TRAIN] Epoch: 17, Iter: 0, Loss: 1.62319\n","[TRAIN] Epoch: 17, Iter: 100, Loss: 1.62588\n","[TRAIN] Epoch: 17, Iter: 200, Loss: 1.50298\n","[TRAIN] Epoch: 17, Iter: 300, Loss: 1.59718\n","[TRAIN] Epoch: 17, Iter: 400, Loss: 1.50009\n","[TRAIN] Epoch: 17, Iter: 500, Loss: 1.57908\n","== [TRAIN] Epoch: 17, Accuracy: 0.316 ==>\n","[VAL] Epoch: 17, Iter: 0, Loss: 1.39029\n","=== [VAL] Epoch: 17, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 18 ======>\n","[TRAIN] Epoch: 18, Iter: 0, Loss: 1.49851\n","[TRAIN] Epoch: 18, Iter: 100, Loss: 1.42039\n","[TRAIN] Epoch: 18, Iter: 200, Loss: 2.13572\n","[TRAIN] Epoch: 18, Iter: 300, Loss: 1.43245\n","[TRAIN] Epoch: 18, Iter: 400, Loss: 1.25580\n","[TRAIN] Epoch: 18, Iter: 500, Loss: 1.56468\n","== [TRAIN] Epoch: 18, Accuracy: 0.316 ==>\n","[VAL] Epoch: 18, Iter: 0, Loss: 1.39006\n","=== [VAL] Epoch: 18, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 19 ======>\n","[TRAIN] Epoch: 19, Iter: 0, Loss: 1.60604\n","[TRAIN] Epoch: 19, Iter: 100, Loss: 1.92597\n","[TRAIN] Epoch: 19, Iter: 200, Loss: 1.41231\n","[TRAIN] Epoch: 19, Iter: 300, Loss: 1.69430\n","[TRAIN] Epoch: 19, Iter: 400, Loss: 1.55387\n","[TRAIN] Epoch: 19, Iter: 500, Loss: 1.55202\n","== [TRAIN] Epoch: 19, Accuracy: 0.316 ==>\n","[VAL] Epoch: 19, Iter: 0, Loss: 1.39596\n","=== [VAL] Epoch: 19, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 20 ======>\n","[TRAIN] Epoch: 20, Iter: 0, Loss: 2.45833\n","[TRAIN] Epoch: 20, Iter: 100, Loss: 1.38387\n","[TRAIN] Epoch: 20, Iter: 200, Loss: 1.48655\n","[TRAIN] Epoch: 20, Iter: 300, Loss: 1.74940\n","[TRAIN] Epoch: 20, Iter: 400, Loss: 1.41522\n","[TRAIN] Epoch: 20, Iter: 500, Loss: 1.37625\n","== [TRAIN] Epoch: 20, Accuracy: 0.316 ==>\n","[VAL] Epoch: 20, Iter: 0, Loss: 1.38790\n","=== [VAL] Epoch: 20, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 21 ======>\n","[TRAIN] Epoch: 21, Iter: 0, Loss: 1.52118\n","[TRAIN] Epoch: 21, Iter: 100, Loss: 1.52939\n","[TRAIN] Epoch: 21, Iter: 200, Loss: 1.89614\n","[TRAIN] Epoch: 21, Iter: 300, Loss: 1.41325\n","[TRAIN] Epoch: 21, Iter: 400, Loss: 1.79399\n","[TRAIN] Epoch: 21, Iter: 500, Loss: 1.38421\n","== [TRAIN] Epoch: 21, Accuracy: 0.316 ==>\n","[VAL] Epoch: 21, Iter: 0, Loss: 1.38869\n","=== [VAL] Epoch: 21, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 22 ======>\n","[TRAIN] Epoch: 22, Iter: 0, Loss: 1.82576\n","[TRAIN] Epoch: 22, Iter: 100, Loss: 1.61426\n","[TRAIN] Epoch: 22, Iter: 200, Loss: 1.54607\n","[TRAIN] Epoch: 22, Iter: 300, Loss: 1.74707\n","[TRAIN] Epoch: 22, Iter: 400, Loss: 1.68089\n","[TRAIN] Epoch: 22, Iter: 500, Loss: 1.66921\n","== [TRAIN] Epoch: 22, Accuracy: 0.316 ==>\n","[VAL] Epoch: 22, Iter: 0, Loss: 1.38751\n","=== [VAL] Epoch: 22, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 23 ======>\n","[TRAIN] Epoch: 23, Iter: 0, Loss: 1.50000\n","[TRAIN] Epoch: 23, Iter: 100, Loss: 1.69219\n","[TRAIN] Epoch: 23, Iter: 200, Loss: 1.88059\n","[TRAIN] Epoch: 23, Iter: 300, Loss: 1.39563\n","[TRAIN] Epoch: 23, Iter: 400, Loss: 1.45373\n","[TRAIN] Epoch: 23, Iter: 500, Loss: 1.48232\n","== [TRAIN] Epoch: 23, Accuracy: 0.316 ==>\n","[VAL] Epoch: 23, Iter: 0, Loss: 1.38730\n","=== [VAL] Epoch: 23, Iter: 71, Accuracy: 0.302 ===>\n","====== Epoch 24 ======>\n","[TRAIN] Epoch: 24, Iter: 0, Loss: 1.38549\n","[TRAIN] Epoch: 24, Iter: 100, Loss: 1.87523\n","[TRAIN] Epoch: 24, Iter: 200, Loss: 1.53667\n","[TRAIN] Epoch: 24, Iter: 300, Loss: 1.70530\n","[TRAIN] Epoch: 24, Iter: 400, Loss: 1.36344\n","[TRAIN] Epoch: 24, Iter: 500, Loss: 1.53500\n","== [TRAIN] Epoch: 24, Accuracy: 0.316 ==>\n","[VAL] Epoch: 24, Iter: 0, Loss: 1.38363\n","=== [VAL] Epoch: 24, Iter: 71, Accuracy: 0.302 ===>\n","[TEST] Epoch: 24, Iter: 0, Loss: 1.66997\n","[TEST] Epoch: 24, Iter: 100, Loss: 1.45769\n","=== [TEST] Epoch: 24, Iter: 143, Accuracy: 0.333 ===>\n"]}],"source":["from tqdm import tqdm\n","\n","train_losses, valid_losses = [], []\n","train_accs, valid_accs = [], []\n","train_times, valid_times = [], []\n","\n","for epoch in range(epochs):\n","    tqdm.write(f\"====== Epoch {epoch} ======>\")\n","    loss, acc, wall_time = train(epoch, model, train_loader, optimizer)\n","    train_losses.append(loss)\n","    train_accs.append(acc)\n","    train_times.append(wall_time)\n","\n","    loss, acc, wall_time = evaluate(epoch, model, val)\n","    valid_losses.append(loss)\n","    valid_accs.append(acc)\n","    valid_times.append(wall_time)\n","\n","test_loss, test_acc, test_time = evaluate(\n","    epoch, model, test, mode=\"test\"\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPVeF9ttQ6ID3pfErocKD3n","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
